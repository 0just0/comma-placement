{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get WikiText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dtemnov/Projects/comma_placement/.venv/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by mode='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", 'wikitext-103-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wikitext contains a lot of delimiters and empty lines, so we need to clean it up a little bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_remover = r'\\s([,?.!:;](?:\\s|$))'\n",
    "quote_space_remover = r'\"\\s*([^\"]*?)\\s*\"'\n",
    "\n",
    "def remove_spaces(samples):\n",
    "    samples = [sample.strip() for sample in samples]\n",
    "    samples = [re.sub(space_remover, r'\\1', sample) for sample in samples]\n",
    "    samples = [re.sub(quote_space_remover, r'\"\\1\"', sample) for sample in samples]\n",
    "    samples = [sample.replace(\" '\", \"'\") for sample in samples]\n",
    "    return samples\n",
    "\n",
    "def remove_empty(samples: list) -> list:\n",
    "    samples = [sample for sample in samples if sample]\n",
    "    return samples\n",
    "\n",
    "def remove_titles(samples: list) -> list:\n",
    "    samples = [sample for sample in samples if \"=\" not in sample]\n",
    "    return samples\n",
    "\n",
    "def remove_short(samples: list) -> list:\n",
    "    samples = [sample for sample in samples if len(sample) > 300]\n",
    "    return samples\n",
    "\n",
    "def remove_unk(samples: list) -> list:\n",
    "    samples = [sample for sample in samples if \"<unk>\" not in sample]\n",
    "    return samples\n",
    "\n",
    "def preprocess_texts(samples: list) -> list:\n",
    "    samples = remove_titles(samples)\n",
    "    samples = remove_spaces(samples)\n",
    "    samples = remove_empty(samples)\n",
    "    samples = remove_short(samples)\n",
    "    samples = remove_unk(samples)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = dataset[\"train\"][0:100000][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = preprocess_texts(dataset_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24656"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final total size of the dataset is about 35k samples. This amount of data should be enough for training and validation for this pretty simple task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/raw_wiki_lines.txt\", \"w\") as f:\n",
    "    f.writelines([sample+\"\\n\" for sample in samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize and add labels for Token classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import json\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/raw/raw_wiki_lines.txt\", \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want the following structure for dataset:\n",
    "\n",
    "Our Labels: \n",
    "ID2LABEL = {0: \"O\", 1: \"B-COMMA\"}\n",
    "sample = {\"tokens\": [\"token1\", \"token2,\", \"token3\"], \"tags\": [0, 1, 0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_sample(sentence: str) -> dict:\n",
    "    sentence = sentence.strip()\n",
    "    words = [word.text for word in nlp(sentence)]\n",
    "    tags = []\n",
    "    clean_words = []\n",
    "    for i in range(len(words)-1):\n",
    "        if words[i] == ',':\n",
    "            continue\n",
    "        if words[i+1] == \",\":\n",
    "            clean_words.append(words[i])\n",
    "            tags.append(1)\n",
    "        else:\n",
    "            clean_words.append(words[i])\n",
    "            tags.append(0)\n",
    "    clean_words.append(words[-1])\n",
    "    tags.append(0)\n",
    "    assert len(tags) == len(clean_words)\n",
    "    return json.dumps({\"tokens\": clean_words, \"tags\": tags})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "122918"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_samples = []\n",
    "for i in data:\n",
    "    sentences = i.split(\".\")\n",
    "    for j in sentences:\n",
    "        sentence = j.strip()\n",
    "        if len(sentence.split()) < 10:\n",
    "            continue\n",
    "        formatted_samples.append(sentence_to_sample(j+\".\"))\n",
    "\n",
    "len(formatted_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/wiki_data.json\", \"w\") as f:\n",
    "    f.writelines([sample+\"\\n\" for sample in formatted_samples])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's now split the formated wiki sentences into train, val, test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed/wiki_data.json\", \"r\") as f:\n",
    "    data = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_lines, test_lines = train_test_split(data, test_size=.16, random_state=42)\n",
    "train_lines, val_lines = train_test_split(train_lines, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, lines in zip([\"train\", \"validation\", \"test\"], [train_lines, val_lines, test_lines]):\n",
    "    with open(f\"../data/processed/wiki_data_{step}.json\", \"w\") as f:\n",
    "        f.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 3/3 [00:00<00:00, 7570.95it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 365.45it/s]\n",
      "Generating train split: 82600 examples [00:00, 564409.04 examples/s]\n",
      "Generating validation split: 20651 examples [00:00, 614027.58 examples/s]\n",
      "Generating test split: 19667 examples [00:00, 969482.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset_structure = {\n",
    "    \"train\": \"../data/processed/wiki_data_train.json\",\n",
    "    \"validation\": \"../data/processed/wiki_data_validation.json\",\n",
    "    \"test\": \"../data/processed/wiki_data_test.json\",\n",
    "}\n",
    "processed_dataset = load_dataset(\"json\", data_files=dataset_structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 82600/82600 [00:00<00:00, 645010.84 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 20651/20651 [00:00<00:00, 777583.42 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 19667/19667 [00:00<00:00, 720538.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "processed_dataset.save_to_disk(\"../data/processed/wiki_comma_placement\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's upload the processed dataset to Hugginface for later usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from huggingface_hub import notebook_login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83e992e6cfcb4331959006b51090bdd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = load_from_disk(\"../data/processed/wiki_comma_placement\")\n",
    "dd.push_to_hub(\"wiki-comma-placement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
